{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c116e7",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "Step 1: run Topic modelling\n",
    "\n",
    "Step 2: Content analysis, Network analysis, Buzz graph??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108b802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import webbrowser\n",
    "import string\n",
    "import gensim  # the library for Topic modelling\n",
    "import pandas as pd\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim import corpora, models\n",
    "import pyLDAvis.gensim  # LDA visualization library\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from itertools import chain  # To merge multiple lists into a single list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23481b80",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057eb98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instagram_df = pd.read_csv(\n",
    "    '/Users/wei/Documents/CARA Network/AMR /AMR Instagram data/all_Instagram_data(non-English excluded).csv')\n",
    "new_df = all_instagram_df\n",
    "\n",
    "\n",
    "# new_df = pd.read_csv(\n",
    "#     '/Users/wei/Job Application 2023/CARA Network/AMR /AMR Instagram data/Antimicrobial resistance/Antimicrobial '\n",
    "#     'resistance 01 Jan 2017 - 01 July 2023_specific hashtags.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d1d44",
   "metadata": {},
   "source": [
    "# Step 1: clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be28afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "custom_stop_words = {'your', 'I', 'for', 'and', 'the', 'to', 'or', 'in', 'of', 'my'}\n",
    "all_stop_words = stop.union(custom_stop_words)\n",
    "\"\"\"Examples of stopwords include \"the,\" \"a,\" \"an,\" \"in,\" \"on,\" etc. The stopwords module from the nltk library\n",
    "provides a list of common stopwords in different languages, and here we are using the ones for English\"\"\"\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\"\"\"For example, the lemma of the words \"running,\" \"runs,\" and \"ran\" is \"run.\" The WordNetLemmatizer class uses the\n",
    "WordNet lexical database to perform lemmatization. This helps reduce inflected words to a common base form,\n",
    "which can be useful for text analysis and processing tasks\"\"\"\n",
    "\n",
    "\n",
    "def clean(text: str) -> list[str]:\n",
    "    try:\n",
    "        if text is None:\n",
    "            raise ValueError(\"Input 'text' is None.\")\n",
    "        # remove hashtags\n",
    "        text_without_hashtags = ' '.join([word for word in text.lower().split() if not word.startswith('#')])\n",
    "        english_vocab = set(words.words())\n",
    "        words_only_english = [word for word in text.split() if word.lower() in english_vocab]\n",
    "\n",
    "        # Non-hashtags text processing2\n",
    "        stop_free = [word for word in text_without_hashtags.split() if word not in all_stop_words]\n",
    "        punc_free = [ch for ch in stop_free if ch not in exclude]\n",
    "        normalized = [lemma.lemmatize(word) for word in punc_free]\n",
    "        # Combine normalized with words_only_english\n",
    "        normalized.extend(words_only_english)\n",
    "\n",
    "        return normalized  # no stopwords, no punc, no hashtags\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred while processing text:\", text)\n",
    "        print(\"Error message:\", str(e))\n",
    "        return []  # return an empty list if an error occurs\n",
    "\n",
    "\n",
    "# try:\n",
    "#     new_df['Caption_cleaned'] = new_df['Caption'].apply(clean)\n",
    "# except Exception as e:\n",
    "#     print(\"Error occurred during 'apply' operation.\")\n",
    "#     print(\"Error message:\", str(e))\n",
    "\n",
    "new_df['Caption_cleaned'] = new_df['Caption'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0a6e8",
   "metadata": {},
   "source": [
    "# Step 2: Create a dictionary from new_df['Caption_cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e8de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(new_df['Caption_cleaned'])\n",
    "# print(dictionary) --> 1925 unique words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d081fca",
   "metadata": {},
   "source": [
    "# Step 3: Create document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a492553",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in new_df['Caption_cleaned']]\n",
    "# print(doc_term_matrix) --> calculate a word  show how many times\n",
    "# The doc2bow function: Convert each text in new_df['Caption_clean'] to document-term representation.\n",
    "# print(dictionary.num_nnz) --> non-repeated words\n",
    "# print(len(doc_term_matrix)) --> a total words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16433a0e",
   "metadata": {},
   "source": [
    "# Step 4: Instantiate LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc340fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel\n",
    "\"\"\"This algorithm assumes that each document in the text is composed of different proportions of topics,\n",
    "and each topic is composed of different proportions of words. LDA finds these latent topics and their word\n",
    "combinations through an iterative process\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc43580",
   "metadata": {},
   "source": [
    "# Step 5: print the topics identified by LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e81074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't overlapping the circle (see on the web)--> If overlapped--> not a good model fit --> shorter the num_topics\n",
    "num_topics = 8  # num_topics: The number of topics to be identified by the LDA model\n",
    "ldamodel = lda(doc_term_matrix, num_topics=num_topics, id2word=dictionary, passes=50, minimum_probability=0,\n",
    "               random_state=50)\n",
    "\"\"\"id2word: The dictionary created in Step 2, which maps word IDs to words.\n",
    "passes: the number of times the algorithm goes through all the documents in the dataset during the training process.\n",
    "Each pass allows the model to learn and update its understanding of the data, potentially improving the quality of the\n",
    "identified topics.\n",
    "minimum_probability:  The minimum probability value required for a word to be considered in a topic.\n",
    "In this case, it's set to 0, meaning all words will be included in the topics regardless of\n",
    "their probability. If set to a higher value (e.g., 0.01), the model will only include words with a probability\n",
    "greater than or equal to the specified value.\n",
    "random_state: \"\"\"\n",
    "# print(ldamodel.print_topics(num_topics=num_topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b04e30",
   "metadata": {},
   "source": [
    "# Step 6: Visualize the LDA model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27976567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary, sort_topics=False, mds='mmds')\n",
    "# save to HTML that can open on web\n",
    "pyLDAvis.save_html(lda_display, 'LDA_Visualization.html')\n",
    "webbrowser.open('file://' + os.path.realpath('LDA_Visualization.html'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a015dc",
   "metadata": {},
   "source": [
    "# Step 7: Find which articles were marked in which cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e947f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns the topics to the documents in corpus\n",
    "topic_distribution = ldamodel[doc_term_matrix]  # contains the topic distribution for each document.\n",
    "# print([doc for doc in topic_distribution]) --> This result shows the topic distribution for each document. Each\n",
    "# document is represented by a list, where each element in the list represents a topic along with its corresponding\n",
    "# probability.\n",
    "scores = list(chain(*[[score for topic_id, score in topic] for topic in [doc for doc in topic_distribution]]))\n",
    "# This line extracts the probability scores for all topics for each document and stores them in the scores list\n",
    "threshold = sum(scores) / len(scores)\n",
    "\"\"\"The threshold is calculated as the average of all probability scores. It's used as a threshold to determine which\n",
    "articles belong to which cluster. Articles with a probability score greater than this threshold are considered to\n",
    "belong to a cluster\"\"\"\n",
    "# print(threshold)\n",
    "\"\"\"After computing the threshold, you can use it to filter out topics that have probability scores below this\n",
    "threshold. Topics with probabilities lower than the threshold are considered less significant or relevant,\n",
    "and you may choose to exclude them from further analysis or visualization. This threshold can help you focus on the\n",
    "most important and representative topics in your topic modeling results.\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Each Threshold\n",
    "1. AMR: 0.8284\n",
    "2. Antimicrobial resistance: 0.7608\n",
    "3. Antibiotics: 0.7205\n",
    "4. Antimicrobials: 0.7420\n",
    "5. Antimicrobial stewardship: 0.8535\n",
    "6. Drug resistant: 0.8302\n",
    "7. Superbugs: 0.8308\n",
    "8. Antibiotic resistance: 0.7203\n",
    "9. Infections: 0.6630\n",
    "10. Bacterial infections:  0.9411\n",
    "11. Antibiotic prescribing: 0.7825\n",
    "\"\"\"\n",
    "\n",
    "# Create a list to store the document IDs for each cluster\n",
    "clusters = [[] for _ in range(num_topics)]\n",
    "\n",
    "# Assign each document to its corresponding cluster based on the dominant topic\n",
    "for doc_index, doc_topics in enumerate(topic_distribution):\n",
    "    if doc_topics:  # Check if doc_topics is not empty\n",
    "        dominant_topic = max(doc_topics, key=lambda x: x[1])  # Find the dominant topic and its probability\n",
    "        if dominant_topic[1] > threshold:  # Check if the probability of the dominant topic is above the threshold\n",
    "            cluster_index = dominant_topic[0]  # Get the index of the dominant topic\n",
    "            clusters[cluster_index].append(doc_index)  # Add the document index to the corresponding cluster\n",
    "        cluster_1_df = new_df.loc[clusters[0]]\n",
    "        cluster_2_df = new_df.loc[clusters[1]]\n",
    "        cluster_3_df = new_df.loc[clusters[2]]\n",
    "        cluster_4_df = new_df.loc[clusters[3]]\n",
    "        cluster_5_df = new_df.loc[clusters[4]]\n",
    "        cluster_6_df = new_df.loc[clusters[5]]\n",
    "        cluster_7_df = new_df.loc[clusters[6]]\n",
    "        cluster_8_df = new_df.loc[clusters[7]]\n",
    "\n",
    "        all_clusters_df = pd.concat([cluster_1_df, cluster_2_df, cluster_3_df, cluster_4_df, cluster_5_df,\n",
    "                                     cluster_6_df, cluster_7_df, cluster_8_df])\n",
    "        all_clusters_df.to_csv(\n",
    "            \"/Users/wei/Job Application 2023/CARA Network/AMR /AMR Instagram data/all Clusters.csv\",\n",
    "            index=False)\n",
    "#         cluster_1_df.to_csv(\n",
    "#             \"/Users/wei/Job Application 2023/CARA Network/AMR /AMR Instagram data/Antimicrobial resistance/Antimicrobial resistance_Cluster 1.csv\",\n",
    "#             index=False)\n",
    "#         cluster_2_df.to_csv(\n",
    "#             \"/Users/wei/Job Application 2023/CARA Network/AMR /AMR Instagram data/Antimicrobial resistance/Antimicrobial resistance_Cluster 2.csv\",\n",
    "#             index=False)\n",
    "#         cluster_3_df.to_csv(\n",
    "#             \"/Users/wei/Job Application 2023/CARA Network/AMR /AMR Instagram data/Antimicrobial resistance/Antimicrobial resistance_Cluster 3.csv\",\n",
    "#             index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fed0bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycara",
   "language": "python",
   "name": "pycara"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
